{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "from scaling import *\n",
    "from feature_selectors import *\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin our model testing by splitting data into train and test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/train.csv')\n",
    "processing_pipeline = joblib.load('../pipelines/text_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target']\n",
    "X = df.drop(columns=['target'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>count_caps_lock</th>\n",
       "      <th>count_exclamation_mark</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_punctuation</th>\n",
       "      <th>count_links</th>\n",
       "      <th>...</th>\n",
       "      <th>joy</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>disgust</th>\n",
       "      <th>anger</th>\n",
       "      <th>surprise</th>\n",
       "      <th>fear</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_text_str</th>\n",
       "      <th>mention_god_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[new, weapon, cause, unimaginable, destruction]</td>\n",
       "      <td>new weapon cause unimaginable destruction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[famp, thing, gishwhes, get, soak, deluge, go,...</td>\n",
       "      <td>famp thing gishwhes get soak deluge go pad tam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dt, rt, ûïthe, col, police, catch, pickpocket...</td>\n",
       "      <td>dt rt ûïthe col police catch pickpocket liverp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[aftershock, back, school, kick, great, want, ...</td>\n",
       "      <td>aftershock back school kick great want thank e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[response, trauma, child, addict, develop, def...</td>\n",
       "      <td>response trauma child addict develop defensive...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>2640</td>\n",
       "      <td>crashed</td>\n",
       "      <td>Somewhere</td>\n",
       "      <td>@SmusX16475 Skype just crashed u host</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[skype, crash, u, host]</td>\n",
       "      <td>skype crash u host</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>731</td>\n",
       "      <td>attacked</td>\n",
       "      <td>Arundel</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>christian attack muslim temple mount wave isra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>5131</td>\n",
       "      <td>fatal</td>\n",
       "      <td>New South Wales, Australia</td>\n",
       "      <td>Man charged over fatal crash near Dubbo refuse...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[man, charge, fatal, crash, near, dubbo, refus...</td>\n",
       "      <td>man charge fatal crash near dubbo refuse bail via</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6740</th>\n",
       "      <td>9657</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#usNWSgov Severe Weather Statement issued Augu...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[usnwsgov, severe, weather, statement, issue, ...</td>\n",
       "      <td>usnwsgov severe weather statement issue august...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>2361</td>\n",
       "      <td>collapsed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Great British &amp;lt;b&amp;gt;Bake&amp;lt;/b&amp;gt; Off's ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[great, british, ltbgtbakeltbgt, offs, back, d...</td>\n",
       "      <td>great british ltbgtbakeltbgt offs back dorrets...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1523 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword                    location  \\\n",
       "2644  3796   destruction                         NaN   \n",
       "2227  3185        deluge                         NaN   \n",
       "5448  7769        police                          UK   \n",
       "132    191    aftershock                         NaN   \n",
       "6845  9810        trauma       Montgomery County, MD   \n",
       "...    ...           ...                         ...   \n",
       "1835  2640       crashed                   Somewhere   \n",
       "506    731      attacked                    Arundel    \n",
       "3592  5131         fatal  New South Wales, Australia   \n",
       "6740  9657  thunderstorm                         NaN   \n",
       "1634  2361     collapsed                         NaN   \n",
       "\n",
       "                                                   text  count_caps_lock  \\\n",
       "2644  So you have a new weapon that can cause un-ima...                0   \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...                2   \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...                2   \n",
       "132   Aftershock back to school kick off was great. ...                1   \n",
       "6845  in response to trauma Children of Addicts deve...                0   \n",
       "...                                                 ...              ...   \n",
       "1835              @SmusX16475 Skype just crashed u host                0   \n",
       "506   Christian Attacked by Muslims at the Temple Mo...                0   \n",
       "3592  Man charged over fatal crash near Dubbo refuse...                0   \n",
       "6740  #usNWSgov Severe Weather Statement issued Augu...                7   \n",
       "1634  Great British &lt;b&gt;Bake&lt;/b&gt; Off's ba...                1   \n",
       "\n",
       "      count_exclamation_mark  count_hashtags  count_words  count_punctuation  \\\n",
       "2644                       0               0           11                  2   \n",
       "2227                       0               1           21                 10   \n",
       "5448                       0               0           15                 13   \n",
       "132                        0               0           21                  3   \n",
       "6845                       0               0           17                  3   \n",
       "...                      ...             ...          ...                ...   \n",
       "1835                       0               0            6                  1   \n",
       "506                        0               0           18                  9   \n",
       "3592                       0               0           12                  6   \n",
       "6740                       0               1           18                 14   \n",
       "1634                       0               0           13                 27   \n",
       "\n",
       "      count_links  ...  joy  positive  negative  disgust  anger  surprise  \\\n",
       "2644            0  ...    0         0         1        0      1         0   \n",
       "2227            0  ...    0         0         1        0      0         1   \n",
       "5448            1  ...    0         1         0        0      0         1   \n",
       "132             0  ...    0         0         1        0      1         0   \n",
       "6845            0  ...    0         1         1        0      0         0   \n",
       "...           ...  ...  ...       ...       ...      ...    ...       ...   \n",
       "1835            0  ...    1         1         1        1      1         1   \n",
       "506             1  ...    1         1         1        1      1         1   \n",
       "3592            1  ...    0         0         1        0      0         0   \n",
       "6740            1  ...    1         1         1        1      1         1   \n",
       "1634            1  ...    1         1         0        0      0         0   \n",
       "\n",
       "      fear                                     processed_text  \\\n",
       "2644     0    [new, weapon, cause, unimaginable, destruction]   \n",
       "2227     1  [famp, thing, gishwhes, get, soak, deluge, go,...   \n",
       "5448     1  [dt, rt, ûïthe, col, police, catch, pickpocket...   \n",
       "132      0  [aftershock, back, school, kick, great, want, ...   \n",
       "6845     1  [response, trauma, child, addict, develop, def...   \n",
       "...    ...                                                ...   \n",
       "1835     1                            [skype, crash, u, host]   \n",
       "506      1  [christian, attack, muslim, temple, mount, wav...   \n",
       "3592     0  [man, charge, fatal, crash, near, dubbo, refus...   \n",
       "6740     1  [usnwsgov, severe, weather, statement, issue, ...   \n",
       "1634     0  [great, british, ltbgtbakeltbgt, offs, back, d...   \n",
       "\n",
       "                                     processed_text_str  mention_god_related  \n",
       "2644          new weapon cause unimaginable destruction                    0  \n",
       "2227  famp thing gishwhes get soak deluge go pad tam...                    0  \n",
       "5448  dt rt ûïthe col police catch pickpocket liverp...                    0  \n",
       "132   aftershock back school kick great want thank e...                    0  \n",
       "6845  response trauma child addict develop defensive...                    0  \n",
       "...                                                 ...                  ...  \n",
       "1835                                 skype crash u host                    0  \n",
       "506   christian attack muslim temple mount wave isra...                    0  \n",
       "3592  man charge fatal crash near dubbo refuse bail via                    0  \n",
       "6740  usnwsgov severe weather statement issue august...                    0  \n",
       "1634  great british ltbgtbakeltbgt offs back dorrets...                    0  \n",
       "\n",
       "[1523 rows x 33 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text preprocessing and new column adding pipeline\n",
    "processing_pipeline.fit_transform(X_train)\n",
    "processing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test these models and vectorizers:\n",
    "- Models:\n",
    "    - Random Forest\n",
    "    - SVM\n",
    "    - Logistic Regression\n",
    "    - Multinomial NB\n",
    "- Vectorizers:\n",
    "    - Count Vectorizer\n",
    "    - CBow\n",
    "    - Tfidf Vectorizer\n",
    "    - Skipgram Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GensimVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_type='cbow', size=100, window=5, min_count=1, workers=4):\n",
    "        self.model_type = model_type\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        model_type = 0 if self.model_type == 'cbow' else 1\n",
    "        self.model = Word2Vec(X, vector_size=self.size, window=self.window, min_count=self.min_count, workers=self.workers, sg=model_type)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.model.wv[word] for word in sentence if word in self.model.wv] or [np.zeros(self.size)], axis=0) for sentence in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['keyword', 'location', 'processed_text', 'text', 'emotions']\n",
    "X_train = X_train.drop(columns=columns_to_remove)\n",
    "X_test = X_test.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1523 entries, 2644 to 1634\n",
      "Data columns (total 28 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      1523 non-null   int64  \n",
      " 1   count_caps_lock         1523 non-null   int64  \n",
      " 2   count_exclamation_mark  1523 non-null   int64  \n",
      " 3   count_hashtags          1523 non-null   int64  \n",
      " 4   count_words             1523 non-null   int64  \n",
      " 5   count_punctuation       1523 non-null   int64  \n",
      " 6   count_links             1523 non-null   int64  \n",
      " 7   count_stopwords         1523 non-null   int64  \n",
      " 8   count_mentions          1523 non-null   int64  \n",
      " 9   count_verbs             1523 non-null   int64  \n",
      " 10  count_nouns             1523 non-null   int64  \n",
      " 11  count_adjectives        1523 non-null   int64  \n",
      " 12  count_adverbs           1523 non-null   int64  \n",
      " 13  polarity                1523 non-null   float64\n",
      " 14  subjectivity            1523 non-null   float64\n",
      " 15  sadness                 1523 non-null   int64  \n",
      " 16  trust                   1523 non-null   int64  \n",
      " 17  anticip                 1523 non-null   int64  \n",
      " 18  anticipation            1523 non-null   int64  \n",
      " 19  joy                     1523 non-null   int64  \n",
      " 20  positive                1523 non-null   int64  \n",
      " 21  negative                1523 non-null   int64  \n",
      " 22  disgust                 1523 non-null   int64  \n",
      " 23  anger                   1523 non-null   int64  \n",
      " 24  surprise                1523 non-null   int64  \n",
      " 25  fear                    1523 non-null   int64  \n",
      " 26  processed_text_str      1523 non-null   object \n",
      " 27  mention_god_related     1523 non-null   int64  \n",
      "dtypes: float64(2), int64(25), object(1)\n",
      "memory usage: 345.1+ KB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining vectorizers and models we will test\n",
    "vectorizers = {\n",
    "    'Tfidf': TfidfVectorizer(),\n",
    "    'Count': CountVectorizer(),\n",
    "    'Skipgram': GensimVectorizer(model_type='skipgram'),\n",
    "    'CBow': GensimVectorizer(model_type='cbow')\n",
    "}\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "    \n",
    "}\n",
    "\n",
    "# Define parameter grids for models\n",
    "param_grid = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'MultinomialNB': {\n",
    "        'alpha': [0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Vectorizer-specific parameters\n",
    "vectorizer_params = {\n",
    "    'max_features': [1000, 2000, 3000],\n",
    "    'ngram_range': [(1, 1), (1, 2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with Tfidf vectorizer (max_features=1000, ngram_range=(1, 1))\n",
      "Running with Tfidf vectorizer (max_features=1000, ngram_range=(1, 2))\n",
      "Running with Tfidf vectorizer (max_features=2000, ngram_range=(1, 1))\n",
      "Running with Tfidf vectorizer (max_features=2000, ngram_range=(1, 2))\n",
      "Running with Tfidf vectorizer (max_features=3000, ngram_range=(1, 1))\n",
      "Running with Tfidf vectorizer (max_features=3000, ngram_range=(1, 2))\n",
      "Running with Count vectorizer (max_features=1000, ngram_range=(1, 1))\n",
      "Running with Count vectorizer (max_features=1000, ngram_range=(1, 2))\n",
      "Running with Count vectorizer (max_features=2000, ngram_range=(1, 1))\n",
      "Running with Count vectorizer (max_features=2000, ngram_range=(1, 2))\n",
      "Running with Count vectorizer (max_features=3000, ngram_range=(1, 1))\n",
      "Running with Count vectorizer (max_features=3000, ngram_range=(1, 2))\n",
      "Running with Skipgram vectorizer...\n",
      "Skipping MultinomialNB with Skipgram due to negative value constraints.\n",
      "Best parameters found:  {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 20}\n",
      "Best parameters found:  {'kernel': 'linear', 'gamma': 'scale', 'C': 100}\n",
      "Best parameters found:  {'solver': 'liblinear', 'penalty': 'l1', 'C': 10}\n",
      "Running with CBow vectorizer...\n",
      "Skipping MultinomialNB with CBow due to negative value constraints.\n",
      "Best parameters found:  {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 10}\n",
      "Best parameters found:  {'kernel': 'rbf', 'gamma': 'scale', 'C': 100}\n",
      "Best parameters found:  {'solver': 'liblinear', 'penalty': 'l1', 'C': 100}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Assume X_train, X_test, y_train, y_test are already defined and preprocessed\n",
    "for vec_name, vectorizer in vectorizers.items():\n",
    "    if vec_name in ['Tfidf', 'Count']:\n",
    "        for max_feat in vectorizer_params['max_features']:\n",
    "            for ngram in vectorizer_params['ngram_range']:\n",
    "                print(f\"Running with {vec_name} vectorizer (max_features={max_feat}, ngram_range={ngram})\")\n",
    "                \n",
    "                vectorizer.set_params(max_features=max_feat, ngram_range=ngram)\n",
    "                X_train_transformed = vectorizer.fit_transform(X_train['processed_text_str'])\n",
    "                X_test_transformed = vectorizer.transform(X_test['processed_text_str'])\n",
    "                \n",
    "                for model_name, model in models.items():\n",
    "                    #print(f\"Running Randomized Search for {model_name}...\")\n",
    "                    \n",
    "                    params = param_grid[model_name]\n",
    "                    \n",
    "                    # Define the Randomized Search parameter distributions\n",
    "                    param_dist = {key: [value] if isinstance(value, int) else value for key, value in params.items()}\n",
    "                    \n",
    "                    # Perform Randomized Search with reduced number of iterations and folds\n",
    "                    random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=5, cv=3, scoring='f1', n_jobs=-1)\n",
    "                    random_search.fit(X_train_transformed, y_train)\n",
    "                    \n",
    "                    # Get the best estimator\n",
    "                    best_estimator = random_search.best_estimator_\n",
    "                    \n",
    "                    # Predict using the best estimator\n",
    "                    #print(\"Predicting using the best estimator...\")\n",
    "                    predictions = best_estimator.predict(X_test_transformed)\n",
    "                    \n",
    "                    # Generate classification report\n",
    "                    #print(\"Generating classification report...\")\n",
    "                    report = classification_report(y_test, predictions, output_dict=True)\n",
    "                    \n",
    "                    # Customizing the report format or content\n",
    "                    custom_report = {\n",
    "                        'Vectorizer': vec_name,\n",
    "                        'Model': model_name,\n",
    "                        'Max Features': max_feat,\n",
    "                        'N-gram Range': ngram,\n",
    "                        'Accuracy': report['accuracy'],\n",
    "                        'Precision': report['weighted avg']['precision'],\n",
    "                        'Recall': report['weighted avg']['recall'],\n",
    "                        'F1-Score': report['weighted avg']['f1-score']\n",
    "                    }\n",
    "                    \n",
    "                    results.append(custom_report)\n",
    "                    #print(\"Completed.\")\n",
    "    else:\n",
    "        print(f\"Running with {vec_name} vectorizer...\")\n",
    "        X_train_transformed = vectorizer.fit_transform(X_train['processed_text_str'])\n",
    "        X_test_transformed = vectorizer.transform(X_test['processed_text_str'])\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            if model_name == 'MultinomialNB':\n",
    "                print(f\"Skipping {model_name} with {vec_name} due to negative value constraints.\")\n",
    "                continue\n",
    "            \n",
    "            #print(f\"Running Randomized Search for {model_name}...\")\n",
    "            \n",
    "            params = param_grid[model_name]\n",
    "            \n",
    "            # Define the Randomized Search parameter distributions\n",
    "            param_dist = {key: [value] if isinstance(value, int) else value for key, value in params.items()}\n",
    "            \n",
    "            # Perform Randomized Search with reduced number of iterations and folds\n",
    "            random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=5, cv=3, scoring='f1', n_jobs=-1)\n",
    "            random_search.fit(X_train_transformed, y_train)\n",
    "            \n",
    "            # Get the best estimator\n",
    "            best_estimator = random_search.best_estimator_\n",
    "            print(\"Best parameters found: \", random_search.best_params_)\n",
    "            \n",
    "            # Predict using the best estimator\n",
    "            #print(\"Predicting using the best estimator...\")\n",
    "            predictions = best_estimator.predict(X_test_transformed)\n",
    "            \n",
    "            # Generate classification report\n",
    "            #print(\"Generating classification report...\")\n",
    "            report = classification_report(y_test, predictions, output_dict=True)\n",
    "            \n",
    "            # Customizing the report format or content\n",
    "            custom_report = {\n",
    "                'Vectorizer': vec_name,\n",
    "                'Model': model_name,\n",
    "                'Accuracy': report['accuracy'],\n",
    "                'Precision': report['weighted avg']['precision'],\n",
    "                'Recall': report['weighted avg']['recall'],\n",
    "                'F1-Score': report['weighted avg']['f1-score']\n",
    "            }\n",
    "            \n",
    "            results.append(custom_report)\n",
    "            #print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df1 = pd.DataFrame(results)\n",
    "# import ace_tools as tools;tools.display_dataframe_to_user(name=\"Model Testing Results\", dataframe=results_df)\n",
    "\n",
    "sorted_df1 = results_df1.sort_values(by='F1-Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Model</th>\n",
       "      <th>Max Features</th>\n",
       "      <th>N-gram Range</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Count</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.801051</td>\n",
       "      <td>0.805342</td>\n",
       "      <td>0.801051</td>\n",
       "      <td>0.796819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Count</td>\n",
       "      <td>SVM</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.800394</td>\n",
       "      <td>0.803895</td>\n",
       "      <td>0.800394</td>\n",
       "      <td>0.796463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Count</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.797111</td>\n",
       "      <td>0.796543</td>\n",
       "      <td>0.797111</td>\n",
       "      <td>0.795593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>SVM</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.799081</td>\n",
       "      <td>0.801739</td>\n",
       "      <td>0.799081</td>\n",
       "      <td>0.795474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Count</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.796454</td>\n",
       "      <td>0.795815</td>\n",
       "      <td>0.796454</td>\n",
       "      <td>0.795025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.795798</td>\n",
       "      <td>0.796464</td>\n",
       "      <td>0.795798</td>\n",
       "      <td>0.793152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Count</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.795141</td>\n",
       "      <td>0.795224</td>\n",
       "      <td>0.795141</td>\n",
       "      <td>0.792896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.794485</td>\n",
       "      <td>0.795106</td>\n",
       "      <td>0.794485</td>\n",
       "      <td>0.791822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.795141</td>\n",
       "      <td>0.796936</td>\n",
       "      <td>0.795141</td>\n",
       "      <td>0.791805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Tfidf</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.794485</td>\n",
       "      <td>0.796173</td>\n",
       "      <td>0.794485</td>\n",
       "      <td>0.791180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vectorizer                Model  Max Features N-gram Range  Accuracy  \\\n",
       "43      Count  Logistic Regression        3000.0       (1, 1)  0.801051   \n",
       "42      Count                  SVM        3000.0       (1, 1)  0.800394   \n",
       "40      Count        MultinomialNB        3000.0       (1, 1)  0.797111   \n",
       "14      Tfidf                  SVM        2000.0       (1, 2)  0.799081   \n",
       "32      Count        MultinomialNB        2000.0       (1, 1)  0.796454   \n",
       "11      Tfidf  Logistic Regression        2000.0       (1, 1)  0.795798   \n",
       "35      Count  Logistic Regression        2000.0       (1, 1)  0.795141   \n",
       "19      Tfidf  Logistic Regression        3000.0       (1, 1)  0.794485   \n",
       "16      Tfidf        MultinomialNB        3000.0       (1, 1)  0.795141   \n",
       "20      Tfidf        MultinomialNB        3000.0       (1, 2)  0.794485   \n",
       "\n",
       "    Precision    Recall  F1-Score  \n",
       "43   0.805342  0.801051  0.796819  \n",
       "42   0.803895  0.800394  0.796463  \n",
       "40   0.796543  0.797111  0.795593  \n",
       "14   0.801739  0.799081  0.795474  \n",
       "32   0.795815  0.796454  0.795025  \n",
       "11   0.796464  0.795798  0.793152  \n",
       "35   0.795224  0.795141  0.792896  \n",
       "19   0.795106  0.794485  0.791822  \n",
       "16   0.796936  0.795141  0.791805  \n",
       "20   0.796173  0.794485  0.791180  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if adding additional features will improve the performance of the best scoring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline to add new selected features\n",
    "pipeline1 = Pipeline([('first_feature_selection',FirstFeatureSelector())])\n",
    "X_train_1 = pipeline1.fit_transform(X_train)\n",
    "X_test_1 = pipeline1.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['processed_text_str', 'count_caps_lock', 'count_exclamation_mark',\n",
      "       'positive', 'polarity', 'count_links', 'count_mentions', 'count_nouns'],\n",
      "      dtype='object')\n",
      "{'kernel': 'linear', 'gamma': 'scale', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "vectorizer = CountVectorizer(max_features=3000, ngram_range=(1,1))\n",
    "X_text = vectorizer.fit_transform(X_train_1['processed_text_str'])\n",
    "X_text_test = vectorizer.transform(X_test_1['processed_text_str']) # the same transformations for text df\n",
    "print(X_train_1.columns)\n",
    "columns = ['count_caps_lock', 'count_exclamation_mark','positive', 'polarity', 'count_links', 'count_mentions', 'count_nouns']\n",
    "\n",
    "additional_features = csr_matrix(X_train_1[columns].values)\n",
    "additional_features_test = csr_matrix(X_test_1[columns].values)\n",
    "\n",
    "X_combined = hstack([X_text, additional_features])\n",
    "X_test_combined =hstack([X_text_test, additional_features_test])\n",
    "\n",
    "params = param_grid[\"SVM\"]\n",
    "param_dist = {key: [value] if isinstance(value, int) else value for key, value in params.items()}\n",
    "random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=5, cv=3, scoring='f1', n_jobs=-1)\n",
    "random_search.fit(X_combined, y_train)\n",
    "    \n",
    "best_estimator = random_search.best_estimator_\n",
    "print(random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.7517006802721088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "predictions = best_estimator.predict(X_test_combined)\n",
    "print(\"f1 score:\", f1_score(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score after adding new features slightly dropped. It can be caused by other Randomized Search parameters but also new columns can act like a 'noise' for our model. \n",
    "\n",
    "As the performance didn't improve it is better to **keep the model simple**, so we will not add those columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
